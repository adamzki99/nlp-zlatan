{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamzki99/nlp-zlatan/blob/main/nlp_zlatan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Google Drive\n",
        "\n",
        "This notebook is designed to be used together with Google Colab. We start by connecting the notebook to our personal Google Drive."
      ],
      "metadata": {
        "id": "1LcgYJ860QY8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJSvTgIasmNY",
        "outputId": "b776babb-eb91-4f9e-c95c-43f1624cdbb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Be careful to check the you have the same filepath for the dataset in your drive"
      ],
      "metadata": {
        "id": "cqJGw4ve7WSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/nlp-datasets/wizard_of_wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asHbxFDYtSbT",
        "outputId": "708bf2fa-f487-4c21-ad69-203532e38c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/nlp-datasets/wizard_of_wikipedia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the dataset\n",
        "\n",
        "The dataset used is very nested, hard to navigate and just difficult to wrap ones head around. So it is recommended to see this [resource](https://parl.ai/projects/wizard_of_wikipedia/) to get a better undersatnding."
      ],
      "metadata": {
        "id": "aaw3vDKc0NyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('data.json', 'r') as file:\n",
        "    json_data = file.read()\n",
        "    data = json.loads(json_data)\n",
        "\n",
        "print('Datatype:', type(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNAyB_-G0-hl",
        "outputId": "5c99c692-9d14-4c3a-abd9-6f4fde8d45d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datatype: <class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following keys to double check with the [resource](https://parl.ai/projects/wizard_of_wikipedia/) that you have loaded in the right dataset."
      ],
      "metadata": {
        "id": "dqX_nhSb71sX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]['dialog'][0].keys()"
      ],
      "metadata": {
        "id": "NHGPwIx47051"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data exploration"
      ],
      "metadata": {
        "id": "54-z81Lt8Huc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How big is the dataset, and how does it look?"
      ],
      "metadata": {
        "id": "tWrizoRA8Kzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "sVzkpxQ28tWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:5]"
      ],
      "metadata": {
        "id": "O-RWf0ba8w4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0].keys()"
      ],
      "metadata": {
        "id": "eyo9hzPk8zd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]['chosen_topic_passage']"
      ],
      "metadata": {
        "id": "sszRNh3c8047"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dictionary keys of Wizard"
      ],
      "metadata": {
        "id": "D7Ndjzbp87Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]['dialog'][0].keys()"
      ],
      "metadata": {
        "id": "KJfY5f24881T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dictionary keys of Apprentice"
      ],
      "metadata": {
        "id": "RwDO2Uwu8-C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]['dialog'][1].keys()"
      ],
      "metadata": {
        "id": "Ut09Fa9s9DyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dialog example"
      ],
      "metadata": {
        "id": "H6ylLuke9EQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(i, \":\", data[0]['dialog'][i]['text'])"
      ],
      "metadata": {
        "id": "Yve8vd579JEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring uniqe types\n",
        "\n",
        "Exploring how many uniqe \"chosen_topic\"s, \"persona\"s and \"wizard_eval\"s there are in the dataset."
      ],
      "metadata": {
        "id": "YsQgQ1Nk9Nlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics = []\n",
        "personas = []\n",
        "wizardEvals = []\n",
        "\n",
        "for entry in data:\n",
        "\n",
        "  topics.append(entry['chosen_topic'])\n",
        "  personas.append(entry['persona'])\n",
        "  wizardEvals.append(entry['wizard_eval'])\n",
        "\n",
        "# Making the list containing only uniqe items\n",
        "topics = list(set(topics))\n",
        "personas = list(set(personas))\n",
        "wizardEvals = list(set(wizardEvals))\n",
        "\n",
        "print(\"topic:\", len(topics), \"persona:\", len(personas), \"wizard_eval:\", len(wizardEvals))"
      ],
      "metadata": {
        "id": "Bxx3ByNr9RMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### \"Wizard evals\""
      ],
      "metadata": {
        "id": "w2Hfz7cs9ni4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why are there more than 5 different \"wizard_eval\"s? The paper only mentions a rating from 1-5. What are the other 2?"
      ],
      "metadata": {
        "id": "VyBDMyiB9UK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for entry in wizardEvals:\n",
        "  print(wizardEvals[entry] )\n"
      ],
      "metadata": {
        "id": "AKc-L7Fr9Xie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's up with -1 and 0? In paper only ratings from 1 to 5 are mentioned."
      ],
      "metadata": {
        "id": "b_R1N-ew9ZZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How often does each rating occur in \"wizard_eval\"s? Visualize all the different instances in a histogram"
      ],
      "metadata": {
        "id": "WsJgBLJW9d8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "wEval = []\n",
        "\n",
        "for entry in data:\n",
        "    wEval.append(entry['wizard_eval'])\n",
        "\n",
        "plt.hist(wEval, bins=2*len(set(wEval))) #the number of bins can probably be improved to look nicer\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IBLsm5vT9g35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Topics"
      ],
      "metadata": {
        "id": "HEqe88dd9qiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is a topic?\n",
        "\n",
        "topics[:10]"
      ],
      "metadata": {
        "id": "6gV8WL1B9sbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Personas"
      ],
      "metadata": {
        "id": "dhjNrvZP9vET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is a persona?\n",
        "\n",
        "personas[:10]"
      ],
      "metadata": {
        "id": "ANHwT47g9xXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## all-MiniLM-L6-v2\n",
        "\n",
        "This implementation is based on the all-MiniLM-L6-v2 model which is available from [Huggingface](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).\n",
        "\n",
        "The all-MiniLM-L6-v2 is a sentence-transformers model. It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search. This is a later model compared to the one showed in one of the tutorials, but is used more or less in the same way."
      ],
      "metadata": {
        "id": "pkYsVG6t97yI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have selected to pick a BERT model as we wanted to explore the posibility of creating a \"vector database\". The use-case is as follows:\n",
        "\n",
        "From a natural user input, we want to retrive the correct Wikipedia passage. So that the input from the user is as small as possible. \n",
        "\n",
        "The reduction of data input comes from the exlusion of topics etc..  "
      ],
      "metadata": {
        "id": "Esd6ldJR9-iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the data\n",
        "\n",
        "We split up the data into a 80/20 split. We use 80% of the original dataset to perform fine-tuing of the all-MiniLM-L6-v2, the rest is then used for validating the reslut.\n",
        "\n",
        "We aim to have the model being able to search the vector-space with new input and still being able to find the correct Wiki-passage.\n",
        "\n",
        "It is important to note that this is a best case scenario as the input is generated from text that is present in the Wiki-passage."
      ],
      "metadata": {
        "id": "ss6hOr20-CpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_extract_train = {\n",
        "    \"chosen_topic\": [],\n",
        "    \"speaker_passage\": [],\n",
        "    \"checked_sentence\": [],\n",
        "    \"chosen_topic_passage\": []\n",
        "}\n",
        "\n",
        "data_extract_test = {\n",
        "    \"chosen_topic\": [],\n",
        "    \"speaker_passage\": [],\n",
        "    \"checked_sentence\": [],\n",
        "    \"chosen_topic_passage\": []\n",
        "}\n",
        "\n",
        "for i, conversation in enumerate(data):\n",
        "\n",
        "  for j, dialog in enumerate(conversation['dialog']):    \n",
        "\n",
        "    if \"Wizard\" in dialog['speaker']:\n",
        "\n",
        "      checked_sentence = list(dialog['checked_sentence'].values())\n",
        "\n",
        "      if \"no_passages_used\" not in checked_sentence:\n",
        "\n",
        "        if j % 4 == 0:\n",
        "\n",
        "          data_extract_test['chosen_topic'].append(conversation['chosen_topic'])\n",
        "          data_extract_test['speaker_passage'].append(dialog['text'])\n",
        "          data_extract_test['checked_sentence'].append(checked_sentence)\n",
        "          data_extract_test['chosen_topic_passage'].append(conversation['chosen_topic_passage'])\n",
        "\n",
        "        else:\n",
        "      \n",
        "          data_extract_train['chosen_topic'].append(conversation['chosen_topic'])\n",
        "          data_extract_train['speaker_passage'].append(dialog['text'])\n",
        "          data_extract_train['checked_sentence'].append(checked_sentence)\n",
        "          data_extract_train['chosen_topic_passage'].append(conversation['chosen_topic_passage'])\n",
        "\n",
        "extract_train_df = pd.DataFrame(data_extract_train)\n",
        "\n",
        "extract_test_df = pd.DataFrame(data_extract_test)\n",
        "\n",
        "extract_test_df"
      ],
      "metadata": {
        "id": "cF8SPf4J-F93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reducing the size\n",
        "\n",
        "The size of the dataset is too big for the amount of available VRAM on the GPU. Therefor we need to reduce the size of the extracted dataset.\n",
        "\n",
        "We also found that using more data will just result in overfitting the model."
      ],
      "metadata": {
        "id": "5xKqfyp7-Hjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testing_size = len(extract_test_df.index)*0.04\n",
        "testing_size = int(testing_size)\n",
        "\n",
        "training_size = len(extract_train_df.index)*0.04\n",
        "training_size = int(training_size)\n",
        "\n",
        "print(\"Ratio %:\", testing_size/training_size * 100)"
      ],
      "metadata": {
        "id": "4eZP1fnw-JW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_test_df = extract_test_df.sample(testing_size)\n",
        "extract_test_df"
      ],
      "metadata": {
        "id": "J-X3mNIU-KRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_test_df = extract_test_df.sample(testing_size)\n",
        "extract_test_df"
      ],
      "metadata": {
        "id": "FahGbaJ--LQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the model\n"
      ],
      "metadata": {
        "id": "NwMEm7aV-OBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers"
      ],
      "metadata": {
        "id": "f1fjKa-t-UNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "eEyjMKTw-Urn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use the model we need to construct sentence pairs. These pairs consists of a \"user-input\" and a sentence from the Wiki-passage\n",
        "\n",
        "As stated earlier, the \"user-input\" is a genereated human-like input. The input from is genereated from the same sentence as the Wiki-passage which it is matched with. \n",
        "\n",
        "We note that this isn't the best case senario, as it can be interperted as the dataset is traning it self and creates a circle dependence. But we see it as being a \"optimal\" scenario instead."
      ],
      "metadata": {
        "id": "6FnG_Iju-aWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_division(dataframe, sample_size:int):\n",
        "\n",
        "  selected_sentences = []\n",
        "  selected_conversation_topics = []\n",
        "\n",
        "  for c, row in dataframe.sample(sample_size).iterrows():\n",
        "    \n",
        "    selected_conversation_topics.append(row['chosen_topic'])\n",
        "\n",
        "    for resp in row['checked_sentence']:\n",
        "      pair = (row['speaker_passage'], resp)\n",
        "      selected_sentences.append(pair)\n",
        "\n",
        "  return selected_sentences, selected_conversation_topics"
      ],
      "metadata": {
        "id": "X-dRzjvO-fcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_sentences_training, conversation_topics_traning = data_division(extract_train_df, len(extract_train_df.index))"
      ],
      "metadata": {
        "id": "IyUPB9um-gXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_sentences_training[0]"
      ],
      "metadata": {
        "id": "q41yutWw-hne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)\n",
        "\n",
        "# Move model to GPU\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "ufNwHMXo-lE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using mean-pooling\n",
        "\n",
        "Becasue of the variable-length of the input we need to transform the input into a fixed-length representation so we can pass it to our model for traning.\n",
        "\n",
        "The process involves taking the average of all the token embeddings in the sequence. More or less, this is achieved by summing up the embeddings and dividing the sum by the total number of tokens in the sequence.\n",
        "\n",
        "Note that mean-pooling does not consider the positional information or the relative importance of individual tokens within the sequence. In order to combat this we make use of a attetion mask inorder to highlight some importance in the embedding."
      ],
      "metadata": {
        "id": "JBuKCGwN-nG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **mean_pooling** function performs mean pooling on token embeddings while considering an attention mask for correct averaging. It takes *model_output* and *attention_mask* as inputs.\n",
        "\n",
        "The function first extracts the token embeddings from *model_output*. It then expands the attention mask to match the dimensions of the token embeddings. The expanded mask is used to mask out the embeddings that should be ignored.\n",
        "\n",
        "Next, the masked token embeddings are summed along the second dimension (axis 1) to obtain the sum of the embeddings for each token. The attention mask is also summed along the second dimension and clamped to avoid division by zero.\n",
        "\n",
        "Finally, the masked token embeddings are divided by the clamped attention mask sum to compute the mean pooling. The resulting mean-pooled embeddings are returned."
      ],
      "metadata": {
        "id": "OMAN3nll-oyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
      ],
      "metadata": {
        "id": "4_rRmvlh-qS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding is performed in the same way as desrcibes in the documentation for the all-MiniLM-L6-v2 model. One step that has been left out is the normalizaiton of the embedding. \n",
        "\n",
        "The normalization whould provide a list of benefits, such as: improved training and stability, reducing dimensionality, alignment of embedding spaces.\n",
        "\n",
        "The reason that we whould like to perform normalization is to have similarity of meaurements when evaluating the performance of the model."
      ],
      "metadata": {
        "id": "rxUNb6a1-rSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_embedding(documents:list, device, model):\n",
        "\n",
        "  encoded_documents = tokenizer(documents, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "  encoded_documents.to(device)\n",
        "  with torch.no_grad():\n",
        "      model_output_documents = model(**encoded_documents)\n",
        "\n",
        "  # Perform pooling\n",
        "  embedding = mean_pooling(model_output_documents, encoded_documents['attention_mask'])\n",
        "\n",
        "  # Normalize embedding\n",
        "  embedding = F.normalize(embedding, p=2, dim=1)\n",
        "\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "eS41lkFQ-uAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embeddings = perform_embedding(documents = selected_sentences_training, device = device, model = model)"
      ],
      "metadata": {
        "id": "NmHQgHq5-u3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Cluster with Hypertools\n",
        "\n",
        "In order to get a better understanding of the dataset we will use Hypertools to transform the very high dimensional space into a something we can understand. \n",
        "\n",
        "We will generate a 3-dimensional wiev of the embeddings and color in correspondence to k-mean clusters. The amount of clusters corresponds to the amount of topics that is included in the dataset."
      ],
      "metadata": {
        "id": "JHzuW4En-xsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install hypertools"
      ],
      "metadata": {
        "id": "wOnmXyT7-1Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hypertools as hyp\n",
        "\n",
        "n_clusters = len(set(conversation_topics_traning))\n",
        "\n",
        "print(\"Number of clusters:\", n_clusters)\n",
        "\n",
        "hyp.plot(sentence_embeddings.cpu().detach().numpy(), '.', n_clusters = n_clusters)"
      ],
      "metadata": {
        "id": "0-NEMuc0-2KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the index with the model output"
      ],
      "metadata": {
        "id": "3biqsCcI-5gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install hnswlib"
      ],
      "metadata": {
        "id": "ezl-mwuK-_xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to search the embedding vector space we perform **k-nearest neighbors** (KNN) query. This works by creating a index using hnswlib, this is to improve the efficiency of the search.\n",
        "\n",
        "We then perform the embedding process on the query, which in our case is the *speaker_passage*, and calculates the absolute distance to the *k* closest elements in the index."
      ],
      "metadata": {
        "id": "cFbn-C1l_DMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hnswlib\n",
        "\n",
        "# Create the HNSW index\n",
        "index = hnswlib.Index(space='l2', dim=sentence_embeddings.shape[1])\n",
        "index.init_index(max_elements=len(sentence_embeddings), ef_construction=200, M=16)\n",
        "\n",
        "# Add sentence embeddings to the index\n",
        "index.add_items(sentence_embeddings.cpu().numpy())"
      ],
      "metadata": {
        "id": "LbpLtMoy_F5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a similarity search\n",
        "def search_embeddings(query:str, k, device, model):\n",
        "\n",
        "  query_embedding = perform_embedding(documents=query, device=device, model=model)\n",
        "\n",
        "  indexes, distances = index.knn_query(query_embedding.cpu(), k=k)\n",
        "\n",
        "  return indexes[0], distances, query_embedding"
      ],
      "metadata": {
        "id": "G1P7XOTx_Gu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get a random speaker passage from the training dataset just to verify that we can use the model"
      ],
      "metadata": {
        "id": "8WFgWiwF_Iuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_message = list(extract_train_df.sample(1).to_dict()['speaker_passage'].values())[0]\n",
        "message = [random_message]\n",
        "\n",
        "message"
      ],
      "metadata": {
        "id": "iY80Mcda_K7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes, distances, query_embedding = search_embeddings(query=message, k=10, device=device, model=model)\n",
        "\n",
        "print(indexes)"
      ],
      "metadata": {
        "id": "k9CpoPBb_Lsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_subset = []\n",
        "\n",
        "for i, ind in enumerate(indexes):\n",
        "  print(\"Distance:\", distances[0][i], \"\\t\", selected_sentences_training[ind][1])\n",
        "  query_subset.append(selected_sentences_training[ind])"
      ],
      "metadata": {
        "id": "S2qgoTWB_MdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Looking at the result\n",
        "\n",
        "Hypertools has it's limitations, so in order to check the how the results look in comparison to the query embedding we will use *matplotlib.pyplot*.\n",
        "\n",
        "Note: Having a 2-dimensional representation of such a high dimensional vectorspace that the embeddings are isn't optimal. But it is better than nothing ðŸ˜‰."
      ],
      "metadata": {
        "id": "TNWOizm0_N3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_sentences_embedding = perform_embedding(documents=query_subset, device=device, model=model)"
      ],
      "metadata": {
        "id": "ZNTL6Wsk_W4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(sentence_embeddings.cpu()[:,0] , sentence_embeddings.cpu()[:,1], c = '#a9a9a9')\n",
        "plt.scatter(selected_sentences_embedding.cpu()[:,0] , selected_sentences_embedding.cpu()[:,1], c = '#4363d8')\n",
        "plt.scatter(query_embedding.cpu()[:,0] , query_embedding.cpu()[:,1], color = '#ffe119')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5kDOy9zZ_YmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model\n",
        "\n",
        "Now we have extracted the data, finetuned the model, and proved that it works once. Now we will have to prove that it works for more cases. \n",
        "\n",
        "Earlier we set aside 20% of the original data for testing. Becasue we are aming to create something that is working as a vector database, we want to have absolute accuracy and we are not interested in similarity. This is by we have a one-to-one comparison and not a BLEU-evaluation or similar."
      ],
      "metadata": {
        "id": "hEWBS_LN_aBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get traning data\n",
        "selected_sentences_testing, _ = data_division(extract_test_df, len(extract_test_df.index))"
      ],
      "metadata": {
        "id": "9Xlr5j-B_c4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "\n",
        "for _, sentence_pair in enumerate(selected_sentences_testing):\n",
        "\n",
        "  indexes, distances, query_embedding = search_embeddings(query=sentence_pair[0], k=1, device=device, model=model)\n",
        "\n",
        "  results = []\n",
        "  for _, i in enumerate(indexes):\n",
        "\n",
        "    if i >= len(selected_sentences_testing):\n",
        "      break\n",
        "\n",
        "    results.append(selected_sentences_testing[i])\n",
        "  \n",
        "\n",
        "  if sentence_pair in results:\n",
        "    score += 1\n",
        "  \n",
        "print(\"Accuracy:\", score/len(selected_sentences_testing)*100, \"%\")"
      ],
      "metadata": {
        "id": "goSDpO4k_d7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thats is quite bad, how does it look if we use the traning data?"
      ],
      "metadata": {
        "id": "J8UGmL5u_gLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "\n",
        "for _, sentence_pair in enumerate(selected_sentences_training):\n",
        "\n",
        "  indexes, distances, query_embedding = search_embeddings(query=sentence_pair[0], k=1, device=device, model=model)\n",
        "\n",
        "  results = []\n",
        "  for _, i in enumerate(indexes):\n",
        "\n",
        "    if i >= len(selected_sentences_training):\n",
        "      break\n",
        "\n",
        "    results.append(selected_sentences_training[i])\n",
        "  \n",
        "\n",
        "  if sentence_pair in results:\n",
        "    score += 1\n",
        "  \n",
        "print(\"Accuracy:\", score/len(selected_sentences_training)*100, \"%\")"
      ],
      "metadata": {
        "id": "x57fBW2E_fU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the correct Wikipedia passage\n",
        "\n",
        "To wrap it up, we want to find the correct Wiki-passage. This will be perfomed by just finding the passage in the bigger Wiki-passage and presenting it to the user."
      ],
      "metadata": {
        "id": "6xkLw17i_lA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_article(checked_sentence:str, data_extract):\n",
        "\n",
        "  for passage in data_extract['chosen_topic_passage']:\n",
        "\n",
        "    extracted_passage = \"\"\n",
        "\n",
        "    for line in passage:\n",
        "      extracted_passage = extracted_passage + \" \" + line\n",
        "\n",
        "    if extracted_passage.find(checked_sentence) == 1:\n",
        "      \n",
        "      return extracted_passage"
      ],
      "metadata": {
        "id": "i83ubXao_kvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence found:\", query_subset[0][1])\n",
        "\n",
        "# Here the whole data_extract_train is passed in, so it is a lot of uncessesary searing. \n",
        "complete_wiki_passge = find_article(checked_sentence=query_subset[0][1], data_extract=data_extract_train)\n",
        "\n",
        "print(\"Wiki-passage:\", complete_wiki_passge)"
      ],
      "metadata": {
        "id": "9QeE9vzE_nVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doc2Vec Approach"
      ],
      "metadata": {
        "id": "7KEBoNXT_0en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "y9ryJ2tq_7Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install numpy pyblas"
      ],
      "metadata": {
        "id": "Cv9P9tAu_97n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import scipy.linalg\n",
        "from scipy.linalg import blas"
      ],
      "metadata": {
        "id": "uJAAbNXW_75-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the Data"
      ],
      "metadata": {
        "id": "xokomcuwAEpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing the training set\n",
        "\n",
        "We first have to decide which Data we want to use to train the model aka what goal are we trying to achieve.\n",
        "As we want to retrieve the correct passage for each turn we should probably train the model on the passages given and then try to retrieve the chosen passage given a sentence from the dialogue."
      ],
      "metadata": {
        "id": "QGiqTWiDAH5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(data[0][\"chosen_topic_passage\"])"
      ],
      "metadata": {
        "id": "i9p40l7ZANuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we want to take all the sentences from each \"chosen_topic_passage\" and separately use those as the training data."
      ],
      "metadata": {
        "id": "fxRbAD3JASW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passages = [[passage for passage in sample['chosen_topic_passage']] for sample in data]\n",
        "pd.DataFrame(passages[:2])"
      ],
      "metadata": {
        "id": "Eoqez8u8AURz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a nested list of lists, let's unfold that list in a way that the nested entries of those lists are their own entries."
      ],
      "metadata": {
        "id": "S2wCVV7yAV1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "for i in passages:\n",
        "  for entry in i:\n",
        "    sentences.append(entry)\n",
        "    \n",
        "pd.DataFrame(sentences[:10])"
      ],
      "metadata": {
        "id": "ukY0wsCQAXVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check our dataset for duplicates."
      ],
      "metadata": {
        "id": "LnYcOTJJAdBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset with duplicates: {len(sentences)}\")\n",
        "\n",
        "#Let's turn the list into a dictionary and then back into a list to eliminate duplicates\n",
        "unique_sentences = list(dict.fromkeys(sentences))\n",
        "print(f\"Cleaned up Dataset: {len(unique_sentences)}\")"
      ],
      "metadata": {
        "id": "dKgkOzCMAe6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We reduced our dataset to 6% of the original one by removing the duplicates."
      ],
      "metadata": {
        "id": "JURFWCPcAgY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing the test set\n",
        "\n",
        "For the test set we need all the sentences created by the wizard which are based on sentences from Wikipedia articles aka the training set so we can then test the similarity between those sentences and the training set.\n",
        "This way we want to be able to recover the sentence that was used to craft a response given by the wizard.\n",
        "We should also save the actual used sentence in some dictionary linking the response and the used sentence to be able to evaluate the model\n",
        "\n",
        "Let's take a look at the structure of the dialogue using pandas"
      ],
      "metadata": {
        "id": "paizPgGkAuyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_dialog = pd.DataFrame(data[0]['dialog'][:2])\n",
        "df_dialog"
      ],
      "metadata": {
        "id": "4NeXl5yvA7GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_value_from_dict(dictionary):\n",
        "    for _, value in dictionary.items():\n",
        "            return value"
      ],
      "metadata": {
        "id": "PcGnv6rbA5yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dictionary with responses and chosen sentences and list with just responses\n",
        "response_sentence_pairs = {}\n",
        "wizard_resps = []\n",
        "\n",
        "for dialogue in data:\n",
        "  for entry in dialogue['dialog']:\n",
        "    if not 'Wizard' in entry['speaker']: #the apprentice doesn't have any responses based on sentences from training set\n",
        "      continue\n",
        "\n",
        "    if get_value_from_dict(entry['checked_sentence']) == 'no_passages_used' or get_value_from_dict(entry['checked_sentence']) is None:\n",
        "      continue\n",
        "\n",
        "    extracted_text = get_value_from_dict(entry['checked_sentence'])\n",
        "\n",
        "    response_sentence_pairs.update({entry['text']:extracted_text})\n",
        "    wizard_resps.append(entry['text'])"
      ],
      "metadata": {
        "id": "eznTNf25A3wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check our new dictionary."
      ],
      "metadata": {
        "id": "fy0sZ8U7A8nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_items = response_sentence_pairs.items()\n",
        "print(list(dict_items)[:2])"
      ],
      "metadata": {
        "id": "KXxy46bfA-4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the list."
      ],
      "metadata": {
        "id": "ELDsvQVgBA-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wizard_resps[:2]"
      ],
      "metadata": {
        "id": "nAx8GrmABCoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, now we have a list with all the responses given by the wizard and a dictionary linking all the responses to the original source sentences.\n",
        "\n",
        "Do we have any duplicates?"
      ],
      "metadata": {
        "id": "M0E3PwyqBEaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset with duplicates: {len(wizard_resps)}\")\n",
        "\n",
        "#Eliminating a few duplicates\n",
        "unique_resps = list(dict.fromkeys(wizard_resps))\n",
        "print(f\"Cleaned up Dataset: {len(unique_resps)}\")"
      ],
      "metadata": {
        "id": "ma6WC-jXBK3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "It seems so, but just a few. How come we have around ten times more responses, than source sentences?"
      ],
      "metadata": {
        "id": "XnmT34UMBNGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocess the Data"
      ],
      "metadata": {
        "id": "HW7I8CZNBSW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a function for preprocessing our data.\n",
        "\n",
        "Note:\n",
        "\n",
        "- Sadly simple_preprocess removes numbers which would be very useful for retrieval of very specific data.\n",
        "- Also consider taking out stopwords."
      ],
      "metadata": {
        "id": "ILWB9BpjBVhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data,tokens_only=False):\n",
        "  for i, line in enumerate(data):\n",
        "    tokens = gensim.utils.simple_preprocess(line, min_len=2, max_len=20)\n",
        "    if tokens_only:\n",
        "      yield tokens\n",
        "    else:\n",
        "      # For training data, add tags\n",
        "      yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
      ],
      "metadata": {
        "id": "Ov95npBSBRDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus = list(preprocess(unique_sentences))\n",
        "test_corpus = list(preprocess(wizard_resps,tokens_only=True))"
      ],
      "metadata": {
        "id": "qqbqYUdxBew1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of structure of train_corpus and test_corpus."
      ],
      "metadata": {
        "id": "G-KWKNgtBhST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(train_corpus[:10])"
      ],
      "metadata": {
        "id": "Mn3tCZ-iBkfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(test_corpus[:10])"
      ],
      "metadata": {
        "id": "lUtqWw0DBmFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model\n",
        "\n",
        "We instantiate a Doc2Vec model with a vector size of 50 dimensions and iterate over the training corpus 40 times\n",
        "\n",
        "If evaluation with test set is bad, maybe try to decrease min_count to 0, so unique words are not lost"
      ],
      "metadata": {
        "id": "08rt3nZMBoyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=80)"
      ],
      "metadata": {
        "id": "bhrsxjyhBr2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a vocabulary"
      ],
      "metadata": {
        "id": "ROKXILCBBv7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.build_vocab(train_corpus)"
      ],
      "metadata": {
        "id": "Gtn3EdZsBvkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essentially, the vocabulary is a list (accessible via model.wv.index_to_key) of all of the unique words extracted from the training corpus. Additional attributes for each word are available using the model.wv.get_vecattr() method, For example, to see how many times test appeared in the training corpus:"
      ],
      "metadata": {
        "id": "GD-rPxW1B0n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Word 'obama' appeared {model.wv.get_vecattr('obama', 'count')} times in the training corpus.\")"
      ],
      "metadata": {
        "id": "5DNeAHxCBzDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model on the corpus (Took 1 minute with 80 epochs with cleaned up dataset)\n"
      ],
      "metadata": {
        "id": "0HEgcbwzB315"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "metadata": {
        "id": "e27aYdp8B5FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model assessment\n",
        "\n",
        "To assess our new model, weâ€™ll first infer new vectors for each document of the training corpus, compare the inferred vectors with the training corpus, and then returning the rank of the document based on self-similarity\n",
        "\n",
        "Note: *This took 6 minutes to execute with the cleaned up dataset*"
      ],
      "metadata": {
        "id": "_V7gUtd4B7_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ranks = []\n",
        "second_ranks = []\n",
        "for doc_id in range(len(train_corpus)):\n",
        "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
        "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "    rank = [docid for docid, sim in sims].index(doc_id)\n",
        "    ranks.append(rank)\n",
        "\n",
        "    second_ranks.append(sims[1])"
      ],
      "metadata": {
        "id": "acjQi6zPCEae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s count how each document ranks with respect to the training corpus"
      ],
      "metadata": {
        "id": "umd40anRCGMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "counter = collections.Counter(ranks)\n",
        "print(counter)"
      ],
      "metadata": {
        "id": "etwed6Z5CHep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at an example by picking a random document from the corpus and infer a vector from the model"
      ],
      "metadata": {
        "id": "V_B1LjYBCJlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "doc_id = random.randint(0, len(train_corpus) - 1)\n",
        "inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
        "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "\n",
        "print('Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
        "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "\n",
        "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: Â«%sÂ»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
      ],
      "metadata": {
        "id": "H1FTJjYPCNty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice above that the most similar document (usually the same text) is has a similarity score approaching 1.0. However, the similarity score for the second-ranked documents should be significantly lower (assuming the documents are in fact different) and the reasoning becomes obvious when we examine the text itself.\n",
        "\n",
        "\n",
        "We can run the next cell repeatedly to see a sampling other target-document comparisons."
      ],
      "metadata": {
        "id": "hfxZXOtRCdZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "doc_id = random.randint(0, len(train_corpus) - 1)\n",
        "\n",
        "# Compare and print the second-most-similar document\n",
        "sim_id = second_ranks[doc_id]\n",
        "\n",
        "print('Train Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
        "print('Similar Document {}: Â«{}Â»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
      ],
      "metadata": {
        "id": "DoQvshxaCg6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This doesn't really look good. Probably the sentences are too short and thus it doesn't work that well. Also omitting the numbers causes an information loss."
      ],
      "metadata": {
        "id": "RsJuF_s4CrTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on single examples"
      ],
      "metadata": {
        "id": "pGI0QJW0CwUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "doc_id = random.randint(0, len(test_corpus) - 1)\n",
        "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
        "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "\n",
        "\n",
        "# Compare and print the 10 most similar documents from the train corpus\n",
        "print(\"Test Document ({}): Â«{}Â»\\n\".format(doc_id, ' '.join(test_corpus[doc_id])))\n",
        "\n",
        "for index in range(10):\n",
        "    print(f\"{index+1}. {sims[index]}: Â«{' '.join(train_corpus[sims[index][0]].words)}Â»\")\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "#Similarity score of original sentence\n",
        "tokenized_sentence = gensim.utils.simple_preprocess(response_sentence_pairs[wizard_resps[doc_id]], min_len=2, max_len=20)\n",
        "for index in range(len(sims)):\n",
        "  if train_corpus[sims[index][0]].words == tokenized_sentence:\n",
        "    print(f\"Similarity of original sentence: \\n{index+1}. {sims[index]}: Â«{' '.join(train_corpus[sims[index][0]].words)}Â»\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(f\"Untokenized Wizard response: {wizard_resps[doc_id]}\")\n",
        "print(f\"Original source sentence: {response_sentence_pairs[wizard_resps[doc_id]]}\")"
      ],
      "metadata": {
        "id": "5OiL0oaeCuSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating model performance on subset of test data"
      ],
      "metadata": {
        "id": "iDzau6OLDFwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the accuracy of the original sentence being in the top 10 and top 20 most similar sentences with 20% of the test data (takes 2 minutes)"
      ],
      "metadata": {
        "id": "julrSPPRDJ1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create test subset\n",
        "test_subset_corpus = test_corpus\n",
        "\n",
        "#counter that keeps track how often the right source sentence was in the top 10\n",
        "counter_10 = 0\n",
        "#counter that keeps track how often the right source sentence was in the top 20\n",
        "counter_20 = 0\n",
        "\n",
        "test_size = 0.2\n",
        "\n",
        "for i in range(int(len(test_corpus)*test_size)):\n",
        "  doc_id = i\n",
        "\n",
        "  inferred_vector = model.infer_vector(test_subset_corpus[doc_id])\n",
        "\n",
        "  sims = model.dv.most_similar([inferred_vector], topn=20)\n",
        "\n",
        "  tokenized_sentence = gensim.utils.simple_preprocess(response_sentence_pairs[wizard_resps[doc_id]], min_len=2, max_len=20)\n",
        "  \n",
        "  for index in range(len(sims)):\n",
        "    if train_corpus[sims[index][0]].words == tokenized_sentence:\n",
        "      counter_20 += 1\n",
        "      if index <= 10:\n",
        "        counter_10 += 1\n",
        "\n",
        "print(f\"Number of test samples: {int(len(test_corpus)*test_size)}\")\n",
        "\n",
        "print(f\"Number of times sentence was in Top 10: {counter_10}\")\n",
        "print(f\"Number of times sentence was in Top 20: {counter_20}\")\n",
        "\n",
        "print(f\"Accuracy for Top 10: {100*counter_10/int(len(test_corpus)*test_size)}%\")\n",
        "print(f\"Accuracy for Top 20: {100*counter_20/int(len(test_corpus)*test_size)}%\")"
      ],
      "metadata": {
        "id": "kn2ui7YnDIZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}